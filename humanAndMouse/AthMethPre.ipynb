{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !/use/bin/env python\n",
    "# encoding:utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import KFold  \n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import easy_excel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import *\n",
    "import sklearn.ensemble\n",
    "import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sys\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "import subprocess\n",
    "from sklearn.utils import shuffle\n",
    "import itertools\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import sys\n",
    "dataset_name=\"human\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_fasta_file(path):\n",
    "    '''\n",
    "    used for load fasta data and transformd into numpy.array format\n",
    "    '''\n",
    "    fh=open(m6a_benchmark_dataset)\n",
    "    seq=[]\n",
    "    for line in fh:\n",
    "        if line.startswith('>'):\n",
    "            continue\n",
    "        else:\n",
    "            seq.append(line.replace('\\r\\n',''))\n",
    "    fh.close()\n",
    "    matrix_data=np.array([list(e) for e in seq])\n",
    "    return matrix_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def AthMethPre_extract_one_line(data_line):\n",
    "    '''\n",
    "    extract features from one line, such as one m6A sample\n",
    "    '''\n",
    "    A=[0,0,0,1]\n",
    "    U=[0,0,1,0]\n",
    "    C=[0,1,0,0]\n",
    "    G=[1,0,0,0]\n",
    "    N=[0,0,0,0]\n",
    "    feature_representation={\"A\":A,\"U\":U,\"C\":C,\"G\":G,\"N\":N}\n",
    "    beginning=0\n",
    "    end=len(data_line)-1\n",
    "    one_line_feature=[]\n",
    "    alphabet='AUCG'\n",
    "    matrix_two=[\"\".join(e) for e in itertools.product(alphabet, repeat=2)] # AA AU AC AG UU UC ...\n",
    "    matrix_three=[\"\".join(e) for e in itertools.product(alphabet, repeat=3)]# AAA AAU AAC ...\n",
    "    matrix_four=[\"\".join(e) for e in itertools.product(alphabet, repeat=4)]# AAAA AAAU AAAC ...\n",
    "    feature_two=np.zeros(16)\n",
    "    feature_three=np.zeros(64)\n",
    "    feature_four=np.zeros(256)\n",
    "    for index,data in enumerate(data_line):\n",
    "        if index==beginning or index==end:\n",
    "            one_line_feature.extend(feature_representation[\"N\"])\n",
    "        elif data in feature_representation.keys():\n",
    "            one_line_feature.extend(feature_representation[data])\n",
    "        if \"\".join(data_line[index:(index+2)]) in matrix_two and index <= end-1:\n",
    "            feature_two[matrix_two.index(\"\".join(data_line[index:(index+2)]))]+=1\n",
    "        if \"\".join(data_line[index:(index+3)]) in matrix_three and index <= end-2:\n",
    "            feature_three[matrix_three.index(\"\".join(data_line[index:(index+3)]))]+=1\n",
    "        if \"\".join(data_line[index:(index+4)]) in matrix_four and index <=end-3:\n",
    "            feature_four[matrix_four.index(\"\".join(data_line[index:(index+4)]))]+=1\n",
    "    sum_two=np.sum(feature_two)\n",
    "    sum_three=np.sum(feature_three)\n",
    "    sum_four=np.sum(feature_four)\n",
    "    one_line_feature.extend(feature_two/sum_two)\n",
    "    one_line_feature.extend(feature_three/sum_three)\n",
    "    one_line_feature.extend(feature_four/sum_four)\n",
    "    return one_line_feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def AthMethPre_feature_extraction(matrix_data):\n",
    "    final_feature_matrix=[AthMethPre_extract_one_line(e) for e in matrix_data]\n",
    "    return final_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "m6a_benchmark_dataset='data/'+dataset_name+'.fasta'\n",
    "matrix_data=read_fasta_file(m6a_benchmark_dataset)\n",
    "final_feature_matrix=AthMethPre_feature_extraction(matrix_data)\n",
    "pd.DataFrame(final_feature_matrix).to_csv('data/'+dataset_name+'_AthMethPre.csv',header=None,index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !/use/bin/env python\n",
    "# encoding:utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import KFold  \n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import easy_excel\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import *\n",
    "import sklearn.ensemble\n",
    "import joblib\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sys\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import subprocess\n",
    "from sklearn.utils import shuffle\n",
    "import itertools\n",
    "name=dataset_name+\"_AthMethPre\"\n",
    "outputname=name\n",
    "path=\"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance(labelArr, predictArr):\n",
    "    #labelArr[i] is actual value,predictArr[i] is predict value\n",
    "    TP = 0.; TN = 0.; FP = 0.; FN = 0.\n",
    "    for i in range(len(labelArr)):\n",
    "        if labelArr[i] == 1 and predictArr[i] == 1:\n",
    "            TP += 1.\n",
    "        if labelArr[i] == 1 and predictArr[i] == 0:\n",
    "            FN += 1.\n",
    "        if labelArr[i] == 0 and predictArr[i] == 1:\n",
    "            FP += 1.\n",
    "        if labelArr[i] == 0 and predictArr[i] == 0:\n",
    "            TN += 1.\n",
    "    if (TP + FN)==0:\n",
    "        SN=0\n",
    "    else:\n",
    "        SN = TP/(TP + FN) #Sensitivity = TP/P  and P = TP + FN\n",
    "    if (FP+TN)==0:\n",
    "        SP=0\n",
    "    else:\n",
    "        SP = TN/(FP + TN) #Specificity = TN/N  and N = TN + FP\n",
    "    if (TP+FP)==0:\n",
    "        precision=0\n",
    "    else:\n",
    "        precision=TP/(TP+FP)\n",
    "    if (TP+FN)==0:\n",
    "        recall=0\n",
    "    else:\n",
    "        recall=TP/(TP+FN)\n",
    "    GM=math.sqrt(recall*SP)\n",
    "    #MCC = (TP*TN-FP*FN)/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n",
    "    return precision,recall,SN,SP,GM,TP,TN,FP,FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.902654867257\n",
      "[[['svmC:1.8048161116gamma:0.03125', 0.90265486725663713, 1.0, 0.8053097345132744, 0.8053097345132744, 1.0, 0.8973905139421045, 0.89215686274509798, 0.89215686274509798, 0.82102014230716303, '0.9027', 910.0, 220.0, 0.0, 1130.0, 1130.0, 1130.0]]]\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    cross validation\n",
    "\"\"\"\n",
    "\n",
    "datapath ='data/'+dataset_name+'_AthMethPre.csv'\n",
    "train_data = pd.read_csv(datapath, header=None, index_col=None)\n",
    "X = np.array(train_data)\n",
    "Y = list(map(lambda x: 1, xrange(len(train_data) / 2)))\n",
    "Y2 = list(map(lambda x: 0, xrange(len(train_data) / 2)))\n",
    "Y.extend(Y2)\n",
    "Y = np.array(Y)\n",
    "svc = svm.SVC()\n",
    "parameters = {'kernel': ['rbf'], 'C':map(lambda x:2**x,np.linspace(-2,5,28)), 'gamma':map(lambda x:2**x,np.linspace(-5,2,28))}\n",
    "clf = GridSearchCV(svc, parameters, cv=10, n_jobs=-1, scoring='accuracy')\n",
    "clf.fit(X, Y)\n",
    "C=clf.best_params_['C']\n",
    "# joblib.dump(clf,'/home02/chenhuangrong/'+name+'.model')\n",
    "print clf.best_score_\n",
    "gamma=clf.best_params_['gamma']\n",
    "y_predict=cross_val_predict(svm.SVC(kernel='rbf',C=C,gamma=gamma),X,Y,cv=10,n_jobs=12)\n",
    "ROC_AUC_area=\"%0.4f\"%cross_val_score(svm.SVC(kernel='rbf',C=C,gamma=gamma),X,Y,cv=10,n_jobs=12).mean()\n",
    "y_predict_prob=cross_val_predict(svm.SVC(kernel='rbf',C=C,gamma=gamma,probability=True),X,Y,cv=10,n_jobs=12,method='predict_proba')\n",
    "predict_save=[Y.astype(int),y_predict.astype(int),y_predict_prob[:,1]]\n",
    "predict_save=np.array(predict_save).T\n",
    "# pd.DataFrame(predict_save).to_csv(path+name+'_predict.csv',header=None,index=False)\n",
    "ROC_AUC_area=\"%0.4f\"%cross_val_score(svm.SVC(kernel='rbf',C=C,gamma=gamma),X,Y,cv=10,n_jobs=-1).mean()\n",
    "ACC=metrics.accuracy_score(Y,y_predict)\n",
    "precision, recall, SN, SP, GM, TP, TN, FP, FN = performance(Y, y_predict)\n",
    "F1_Score=metrics.f1_score(Y, y_predict)\n",
    "F_measure=F1_Score\n",
    "MCC=metrics.matthews_corrcoef(Y, y_predict)\n",
    "pos=TP+FN\n",
    "neg=FP+TN\n",
    "savedata=[[['svm'+\"C:\"+str(C)+\"gamma:\"+str(gamma),ACC,precision, recall,SN, SP, GM,F_measure,F1_Score,MCC,ROC_AUC_area,TP,FN,FP,TN,pos,neg]]]\n",
    "print savedata\n",
    "print X.shape[1]\n",
    "# easy_excel.save(\"svm_crossvalidation\",[str(X.shape[1])],savedata,'/home02/chenhuangrong/cross_validation_'+name+'.xls')\n",
    "# y_predict_proba=cross_val_predict(svm.SVC(kernel='rbf',C=C,gamma=gamma,probability=True),X,Y,cv=10,method=\"predict_proba\")\n",
    "# Y=pd.DataFrame(Y)    \n",
    "# y_predict_proba=pd.DataFrame(y_predict_proba)\n",
    "# y_predict_proba=pd.concat([Y,y_predict_proba],axis=1)\n",
    "# pd.DataFrame(y_predict_proba).to_csv('/home02/chenhuangrong/RFH_ten_cross_validation_label.csv',header=None,index=False)\n",
    "# y_predict=pd.DataFrame(y_predict)\n",
    "# y_predict_=pd.concat([Y,y_predict],axis=1)\n",
    "# pd.DataFrame(y_predict_).to_csv('/home02/chenhuangrong/RFH_ten_cross_validation_predict.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['GBDTn_estimators:93max_depth:13min_samples_split:500min_samples_leaf:60', 0.90398230088495579, 0.9851222104144527, 0.820353982300885, 0.820353982300885, 0.9876106194690265, 0.9001057185931327, 0.89521970062771616, 0.89521970062771616, 0.81950869923688785, 0.90398230088495568, 927.0, 203.0, 14.0, 1116.0, 1130.0, 1130.0]]]\n",
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    cross validation GBDT\n",
    "\"\"\"\n",
    "classifier=\"GBDT\"\n",
    "datapath =path+outputname+\".csv\"\n",
    "train_data = pd.read_csv(datapath, header=None, index_col=None)\n",
    "X = np.array(train_data)\n",
    "Y = list(map(lambda x: 1, xrange(len(train_data) // 2)))\n",
    "Y2 = list(map(lambda x: 0, xrange(len(train_data) // 2)))\n",
    "Y.extend(Y2)\n",
    "Y = np.array(Y)\n",
    "GBDT = GradientBoostingClassifier(learning_rate=0.1,min_samples_split=300,min_samples_leaf=20,max_depth=8,max_features='sqrt')\n",
    "parameters = {'n_estimators':range(10,120,1),'max_depth':range(3,14,2), 'min_samples_split':range(100,801,200),'min_samples_leaf':range(60,101,10)}\n",
    "clf = GridSearchCV(GBDT, parameters, cv=10, n_jobs=12, scoring='accuracy')\n",
    "clf.fit(X, Y)\n",
    "n_estimators=clf.best_params_['n_estimators']\n",
    "max_depth=clf.best_params_['max_depth']\n",
    "min_samples_split=clf.best_params_['min_samples_split']\n",
    "min_samples_leaf=clf.best_params_['min_samples_leaf']\n",
    "# joblib.dump(clf,'/home02/chenhuangrong/'+name+'.model')\n",
    "# print clf.best_score_\n",
    "y_predict=cross_val_predict(GradientBoostingClassifier(n_estimators=n_estimators,learning_rate=0.1,min_samples_split=min_samples_split,\n",
    "                                                       min_samples_leaf=min_samples_leaf,max_depth=max_depth,max_features='sqrt'),X,Y,cv=10,n_jobs=12)\n",
    "ROC_AUC_area=metrics.roc_auc_score(Y, y_predict)\n",
    "y_predict_prob=cross_val_predict(GradientBoostingClassifier(n_estimators=n_estimators,learning_rate=0.1,min_samples_split=min_samples_split,\n",
    "                                                       min_samples_leaf=min_samples_leaf,max_depth=max_depth,max_features='sqrt'),X,Y,cv=10,n_jobs=12,method='predict_proba')\n",
    "predict_save=[Y.astype(int),y_predict.astype(int),y_predict_prob[:,1]]\n",
    "predict_save=np.array(predict_save).T\n",
    "pd.DataFrame(predict_save).to_csv(path+outputname+\"_\"+classifier+'_predict.csv',header=None,index=False)\n",
    "ACC=metrics.accuracy_score(Y,y_predict)\n",
    "precision, recall, SN, SP, GM, TP, TN, FP, FN = performance(Y, y_predict)\n",
    "F1_Score=metrics.f1_score(Y, y_predict)\n",
    "F_measure=F1_Score\n",
    "MCC=metrics.matthews_corrcoef(Y, y_predict)\n",
    "pos=TP+FN\n",
    "neg=FP+TN\n",
    "savedata=[[[classifier+\"n_estimators:\"+str(n_estimators)+\"max_depth:\"+str(max_depth)+\"min_samples_split:\"+str(min_samples_split)+\"min_samples_leaf:\"+str(min_samples_leaf),ACC,precision, recall,SN, SP,\n",
    "            GM,F_measure,F1_Score,MCC,ROC_AUC_area,TP,FN,FP,TN,pos,neg]]]\n",
    "print savedata\n",
    "print X.shape[1]\n",
    "easy_excel.save(classifier+\"_crossvalidation\",[str(X.shape[1])],savedata,path+'cross_validation_'+classifier+\"_\"+outputname+'.xls')\n",
    "# y_predict_proba=cross_val_predict(svm.SVC(kernel='rbf',C=C,gamma=gamma,probability=True),X,Y,cv=10,method=\"predict_proba\")\n",
    "# Y=pd.DataFrame(Y)    \n",
    "# y_predict_proba=pd.DataFrame(y_predict_proba)\n",
    "# y_predict_proba=pd.concat([Y,y_predict_proba],axis=1)\n",
    "# pd.DataFrame(y_predict_proba).to_csv('/home02/chenhuangrong/RFH_ten_cross_validation_label.csv',header=None,index=False)\n",
    "# y_predict=pd.DataFrame(y_predict)\n",
    "# y_predict_=pd.concat([Y,y_predict],axis=1)\n",
    "# pd.DataFrame(y_predict_).to_csv('/home02/chenhuangrong/RFH_ten_cross_validation_predict.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['RFn_estimators:63max_depth:5min_samples_split:100min_samples_leaf:60', 0.88230088495575221, 0.9705882352941176, 0.7884955752212389, 0.7884955752212389, 0.9761061946902655, 0.8773000715030835, 0.8701171875, 0.8701171875, 0.778423868319296, 0.88230088495575221, 891.0, 239.0, 27.0, 1103.0, 1130.0, 1130.0]]]\n",
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    cross validation RF\n",
    "\"\"\"\n",
    "classifier=\"RF\"\n",
    "datapath =path+outputname+\".csv\"\n",
    "train_data = pd.read_csv(datapath, header=None, index_col=None)\n",
    "X = np.array(train_data)\n",
    "Y = list(map(lambda x: 1, xrange(len(train_data) // 2)))\n",
    "Y2 = list(map(lambda x: 0, xrange(len(train_data) // 2)))\n",
    "Y.extend(Y2)\n",
    "Y = np.array(Y)\n",
    "RF = RandomForestClassifier(min_samples_split=300,min_samples_leaf=20,max_depth=8,max_features='sqrt')\n",
    "parameters = {'n_estimators':range(10,120,1),'max_depth':range(3,14,2), 'min_samples_split':range(100,801,200),'min_samples_leaf':range(60,101,10)}\n",
    "clf = GridSearchCV(RF, parameters, cv=10, n_jobs=12, scoring='accuracy')\n",
    "clf.fit(X, Y)\n",
    "n_estimators=clf.best_params_['n_estimators']\n",
    "max_depth=clf.best_params_['max_depth']\n",
    "min_samples_split=clf.best_params_['min_samples_split']\n",
    "min_samples_leaf=clf.best_params_['min_samples_leaf']\n",
    "# joblib.dump(clf,'/home02/chenhuangrong/'+name+'.model')\n",
    "# print clf.best_score_\n",
    "y_predict=cross_val_predict(RandomForestClassifier(n_estimators=n_estimators,min_samples_split=min_samples_split,\n",
    "                                                       min_samples_leaf=min_samples_leaf,max_depth=max_depth,max_features='sqrt'),X,Y,cv=10,n_jobs=12)\n",
    "ROC_AUC_area=metrics.roc_auc_score(Y, y_predict)\n",
    "y_predict_prob=cross_val_predict(RandomForestClassifier(n_estimators=n_estimators,min_samples_split=min_samples_split,\n",
    "                                                       min_samples_leaf=min_samples_leaf,max_depth=max_depth,max_features='sqrt'),X,Y,cv=10,n_jobs=12,method='predict_proba')\n",
    "predict_save=[Y.astype(int),y_predict.astype(int),y_predict_prob[:,1]]\n",
    "predict_save=np.array(predict_save).T\n",
    "pd.DataFrame(predict_save).to_csv(path+outputname+\"_\"+classifier+'_predict.csv',header=None,index=False)\n",
    "ACC=metrics.accuracy_score(Y,y_predict)\n",
    "precision, recall, SN, SP, GM, TP, TN, FP, FN = performance(Y, y_predict)\n",
    "F1_Score=metrics.f1_score(Y, y_predict)\n",
    "F_measure=F1_Score\n",
    "MCC=metrics.matthews_corrcoef(Y, y_predict)\n",
    "pos=TP+FN\n",
    "neg=FP+TN\n",
    "savedata=[[[classifier+\"n_estimators:\"+str(n_estimators)+\"max_depth:\"+str(max_depth)+\"min_samples_split:\"+str(min_samples_split)+\"min_samples_leaf:\"+str(min_samples_leaf),ACC,precision, recall,SN, SP,\n",
    "            GM,F_measure,F1_Score,MCC,ROC_AUC_area,TP,FN,FP,TN,pos,neg]]]\n",
    "print savedata\n",
    "print X.shape[1]\n",
    "easy_excel.save(classifier+\"_crossvalidation\",[str(X.shape[1])],savedata,path+'cross_validation_'+classifier+\"_\"+outputname+'.xls')\n",
    "# y_predict_proba=cross_val_predict(svm.SVC(kernel='rbf',C=C,gamma=gamma,probability=True),X,Y,cv=10,method=\"predict_proba\")\n",
    "# Y=pd.DataFrame(Y)    \n",
    "# y_predict_proba=pd.DataFrame(y_predict_proba)\n",
    "# y_predict_proba=pd.concat([Y,y_predict_proba],axis=1)\n",
    "# pd.DataFrame(y_predict_proba).to_csv('/home02/chenhuangrong/RFH_ten_cross_validation_label.csv',header=None,index=False)\n",
    "# y_predict=pd.DataFrame(y_predict)\n",
    "# y_predict_=pd.concat([Y,y_predict],axis=1)\n",
    "# pd.DataFrame(y_predict_).to_csv('/home02/chenhuangrong/RFH_ten_cross_validation_predict.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['NB', 0.90176991150442476, 0.970954356846473, 0.8283185840707965, 0.8283185840707965, 0.9752212389380531, 0.8987735397712463, 0.89398280802292251, 0.89398280802292251, 0.81235307884993657, 0.90176991150442476, 936.0, 194.0, 28.0, 1102.0, 1130.0, 1130.0]]]\n",
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    cross validation NB\n",
    "\"\"\"\n",
    "classifier=\"NB\"\n",
    "datapath =path+outputname+\".csv\"\n",
    "train_data = pd.read_csv(datapath, header=None, index_col=None)\n",
    "X = np.array(train_data)\n",
    "Y = list(map(lambda x: 1, xrange(len(train_data) // 2)))\n",
    "Y2 = list(map(lambda x: 0, xrange(len(train_data) // 2)))\n",
    "Y.extend(Y2)\n",
    "Y = np.array(Y)\n",
    "# joblib.dump(clf,'/home02/chenhuangrong/'+name+'.model')\n",
    "# print clf.best_score_\n",
    "y_predict=cross_val_predict(GaussianNB(),X,Y,cv=10,n_jobs=12)\n",
    "ROC_AUC_area=metrics.roc_auc_score(Y, y_predict)\n",
    "y_predict_prob=cross_val_predict(GaussianNB(),X,Y,cv=10,n_jobs=12,method='predict_proba')\n",
    "predict_save=[Y.astype(int),y_predict.astype(int),y_predict_prob[:,1]]\n",
    "predict_save=np.array(predict_save).T\n",
    "pd.DataFrame(predict_save).to_csv(path+outputname+\"_\"+classifier+'_predict.csv',header=None,index=False)\n",
    "ACC=metrics.accuracy_score(Y,y_predict)\n",
    "precision, recall, SN, SP, GM, TP, TN, FP, FN = performance(Y, y_predict)\n",
    "F1_Score=metrics.f1_score(Y, y_predict)\n",
    "F_measure=F1_Score\n",
    "MCC=metrics.matthews_corrcoef(Y, y_predict)\n",
    "pos=TP+FN\n",
    "neg=FP+TN\n",
    "savedata=[[[classifier,ACC,precision, recall,SN, SP, GM,F_measure,F1_Score,MCC,ROC_AUC_area,TP,FN,FP,TN,pos,neg]]]\n",
    "print savedata\n",
    "print X.shape[1]\n",
    "easy_excel.save(classifier+\"_crossvalidation\",[str(X.shape[1])],savedata,path+'cross_validation_'+classifier+\"_\"+outputname+'.xls')\n",
    "# y_predict_proba=cross_val_predict(svm.SVC(kernel='rbf',C=C,gamma=gamma,probability=True),X,Y,cv=10,method=\"predict_proba\")\n",
    "# Y=pd.DataFrame(Y)    \n",
    "# y_predict_proba=pd.DataFrame(y_predict_proba)\n",
    "# y_predict_proba=pd.concat([Y,y_predict_proba],axis=1)\n",
    "# pd.DataFrame(y_predict_proba).to_csv('/home02/chenhuangrong/RFH_ten_cross_validation_label.csv',header=None,index=False)\n",
    "# y_predict=pd.DataFrame(y_predict)\n",
    "# y_predict_=pd.concat([Y,y_predict],axis=1)\n",
    "# pd.DataFrame(y_predict_).to_csv('/home02/chenhuangrong/RFH_ten_cross_validation_predict.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['LRC:1.0', 0.90486725663716816, 0.987220447284345, 0.820353982300885, 0.820353982300885, 0.9893805309734514, 0.9009119039035032, 0.89608506524891263, 0.89608506524891263, 0.82155546537505553, 0.90486725663716816, 927.0, 203.0, 12.0, 1118.0, 1130.0, 1130.0]]]\n",
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    cross validation\n",
    "\"\"\"\n",
    "classifier=\"LR\"\n",
    "datapath =path+outputname+\".csv\"\n",
    "train_data = pd.read_csv(datapath, header=None, index_col=None)\n",
    "X = np.array(train_data)\n",
    "Y = list(map(lambda x: 1, xrange(len(train_data) // 2)))\n",
    "Y2 = list(map(lambda x: 0, xrange(len(train_data) // 2)))\n",
    "Y.extend(Y2)\n",
    "Y = np.array(Y)\n",
    "logisReg = LogisticRegression()\n",
    "C_range = 10.0 ** np.arange(-4,3,1)\n",
    "grid_parame = dict(C=C_range)\n",
    "clf = GridSearchCV(logisReg, grid_parame, cv=10, n_jobs=12, scoring='accuracy')\n",
    "clf.fit(X, Y)\n",
    "C=clf.best_params_['C']\n",
    "# joblib.dump(clf,'/home02/chenhuangrong/'+name+'.model')\n",
    "# print clf.best_score_\n",
    "\n",
    "y_predict=cross_val_predict(LogisticRegression(C=C),X,Y,cv=10,n_jobs=12)\n",
    "ROC_AUC_area=metrics.roc_auc_score(Y, y_predict)\n",
    "y_predict_prob=cross_val_predict(LogisticRegression(C=C),X,Y,cv=10,n_jobs=12,method='predict_proba')\n",
    "predict_save=[Y.astype(int),y_predict.astype(int),y_predict_prob[:,1]]\n",
    "predict_save=np.array(predict_save).T\n",
    "pd.DataFrame(predict_save).to_csv(path+outputname+\"_\"+classifier+'_predict.csv',header=None,index=False)\n",
    "ACC=metrics.accuracy_score(Y,y_predict)\n",
    "precision, recall, SN, SP, GM, TP, TN, FP, FN = performance(Y, y_predict)\n",
    "F1_Score=metrics.f1_score(Y, y_predict)\n",
    "F_measure=F1_Score\n",
    "MCC=metrics.matthews_corrcoef(Y, y_predict)\n",
    "pos=TP+FN\n",
    "neg=FP+TN\n",
    "savedata=[[[classifier+\"C:\"+str(C),ACC,precision, recall,SN, SP, GM,F_measure,F1_Score,MCC,ROC_AUC_area,TP,FN,FP,TN,pos,neg]]]\n",
    "print savedata\n",
    "print X.shape[1]\n",
    "easy_excel.save(classifier+\"_crossvalidation\",[str(X.shape[1])],savedata,path+'cross_validation_'+classifier+\"_\"+outputname+'.xls')\n",
    "# y_predict_proba=cross_val_predict(svm.SVC(kernel='rbf',C=C,gamma=gamma,probability=True),X,Y,cv=10,method=\"predict_proba\")\n",
    "# Y=pd.DataFrame(Y)    \n",
    "# y_predict_proba=pd.DataFrame(y_predict_proba)\n",
    "# y_predict_proba=pd.concat([Y,y_predict_proba],axis=1)\n",
    "# pd.DataFrame(y_predict_proba).to_csv('/home02/chenhuangrong/RFH_ten_cross_validation_label.csv',header=None,index=False)\n",
    "# y_predict=pd.DataFrame(y_predict)\n",
    "# y_predict_=pd.concat([Y,y_predict],axis=1)\n",
    "# pd.DataFrame(y_predict_).to_csv('/home02/chenhuangrong/RFH_ten_cross_validation_predict.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['KNNn_neighbors:9weights:uniformalgorithm:autoleaf_size:1', 0.68495575221238936, 0.8530405405405406, 0.4469026548672566, 0.4469026548672566, 0.9230088495575222, 0.6422578184290394, 0.58652729384436708, 0.58652729384436708, 0.42064662160882549, 0.68495575221238936, 505.0, 625.0, 87.0, 1043.0, 1130.0, 1130.0]]]\n",
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    cross validation\n",
    "\"\"\"\n",
    "classifier=\"KNN\"\n",
    "datapath =path+outputname+\".csv\"\n",
    "train_data = pd.read_csv(datapath, header=None, index_col=None)\n",
    "X = np.array(train_data)\n",
    "Y = list(map(lambda x: 1, xrange(len(train_data) // 2)))\n",
    "Y2 = list(map(lambda x: 0, xrange(len(train_data) // 2)))\n",
    "Y.extend(Y2)\n",
    "Y = np.array(Y)\n",
    "knn = KNeighborsClassifier()\n",
    "#设置k的范围\n",
    "k_range = list(range(1,10))\n",
    "leaf_range = list(range(1,2))\n",
    "weight_options = ['uniform','distance']\n",
    "algorithm_options = ['auto','ball_tree','kd_tree','brute']\n",
    "param_gridknn = dict(n_neighbors = k_range,weights = weight_options,algorithm=algorithm_options,leaf_size=leaf_range)\n",
    "clf = GridSearchCV(knn, param_gridknn, cv=10, n_jobs=12, scoring='accuracy')\n",
    "clf.fit(X, Y)\n",
    "n_neighbors=clf.best_params_['n_neighbors']\n",
    "weights=clf.best_params_['weights']\n",
    "algorithm=clf.best_params_['algorithm']\n",
    "leaf_size=clf.best_params_['leaf_size']\n",
    "# joblib.dump(clf,'/home02/chenhuangrong/'+name+'.model')\n",
    "# print clf.best_score_\n",
    "\n",
    "y_predict=cross_val_predict(KNeighborsClassifier(n_neighbors=n_neighbors,weights=weights,algorithm=algorithm,leaf_size=leaf_size),X,Y,cv=10,n_jobs=12)\n",
    "ROC_AUC_area=metrics.roc_auc_score(Y, y_predict)\n",
    "y_predict_prob=cross_val_predict(KNeighborsClassifier(n_neighbors=n_neighbors,weights=weights,algorithm=algorithm,leaf_size=leaf_size),X,Y,cv=10,n_jobs=12,method='predict_proba')\n",
    "predict_save=[Y.astype(int),y_predict.astype(int),y_predict_prob[:,1]]\n",
    "predict_save=np.array(predict_save).T\n",
    "pd.DataFrame(predict_save).to_csv(path+outputname+\"_\"+classifier+'_predict.csv',header=None,index=False)\n",
    "ACC=metrics.accuracy_score(Y,y_predict)\n",
    "precision, recall, SN, SP, GM, TP, TN, FP, FN = performance(Y, y_predict)\n",
    "F1_Score=metrics.f1_score(Y, y_predict)\n",
    "F_measure=F1_Score\n",
    "MCC=metrics.matthews_corrcoef(Y, y_predict)\n",
    "pos=TP+FN\n",
    "neg=FP+TN\n",
    "savedata=[[[classifier+\"n_neighbors:\"+str(n_neighbors)+\"weights:\"+str(weights)+\"algorithm:\"+str(algorithm)+\"leaf_size:\"+str(leaf_size),\n",
    "            ACC,precision, recall,SN, SP, GM,F_measure,F1_Score,MCC,ROC_AUC_area,TP,FN,FP,TN,pos,neg]]]\n",
    "print savedata\n",
    "print X.shape[1]\n",
    "easy_excel.save(classifier+\"_crossvalidation\",[str(X.shape[1])],savedata,path+'cross_validation_'+classifier+\"_\"+outputname+'.xls')\n",
    "# y_predict_proba=cross_val_predict(svm.SVC(kernel='rbf',C=C,gamma=gamma,probability=True),X,Y,cv=10,method=\"predict_proba\")\n",
    "# Y=pd.DataFrame(Y)    \n",
    "# y_predict_proba=pd.DataFrame(y_predict_proba)\n",
    "# y_predict_proba=pd.concat([Y,y_predict_proba],axis=1)\n",
    "# pd.DataFrame(y_predict_proba).to_csv('/home02/chenhuangrong/RFH_ten_cross_validation_label.csv',header=None,index=False)\n",
    "# y_predict=pd.DataFrame(y_predict)\n",
    "# y_predict_=pd.concat([Y,y_predict],axis=1)\n",
    "# pd.DataFrame(y_predict_).to_csv('/home02/chenhuangrong/RFH_ten_cross_validation_predict.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['XGBoostn_estimators:8max_depth:5learning_rate:0.3subsample:0.85', 0.90796460176991145, 0.9978401727861771, 0.8176991150442477, 0.8176991150442477, 0.9982300884955753, 0.9034665793339414, 0.89883268482490264, 0.89883268482490264, 0.82955944604712928, 0.90796460176991145, 924.0, 206.0, 2.0, 1128.0, 1130.0, 1130.0]]]\n",
      "500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    cross validation RF\n",
    "\"\"\"\n",
    "classifier=\"XGBoost\"\n",
    "datapath =path+outputname+\".csv\"\n",
    "train_data = pd.read_csv(datapath, header=None, index_col=None)\n",
    "X = np.array(train_data)\n",
    "Y = list(map(lambda x: 1, xrange(len(train_data) // 2)))\n",
    "Y2 = list(map(lambda x: 0, xrange(len(train_data) // 2)))\n",
    "Y.extend(Y2)\n",
    "Y = np.array(Y)\n",
    "parameters = [{'n_estimators':range(1,10,1),\n",
    "                      'max_depth':range(2,10),\n",
    "                      'learning_rate':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8],\n",
    "                      'subsample':[0.75,0.8,0.85,0.9]\n",
    "                      }]\n",
    "clf = GridSearchCV(XGBClassifier(), parameters, cv=10, n_jobs=1, scoring='accuracy')\n",
    "clf.fit(X, Y)\n",
    "n_estimators=clf.best_params_['n_estimators']\n",
    "max_depth=clf.best_params_['max_depth']\n",
    "learning_rate=clf.best_params_['learning_rate']\n",
    "subsample=clf.best_params_['subsample']\n",
    "# joblib.dump(clf,'/home02/chenhuangrong/'+name+'.model')\n",
    "# print clf.best_score_\n",
    "y_predict=cross_val_predict(XGBClassifier(n_estimators=n_estimators,learning_rate=learning_rate,\n",
    "                                                       subsample=subsample,max_depth=max_depth),X,Y,cv=10,n_jobs=1)\n",
    "ROC_AUC_area=metrics.roc_auc_score(Y, y_predict)\n",
    "y_predict_prob=cross_val_predict(XGBClassifier(n_estimators=n_estimators,learning_rate=learning_rate,\n",
    "                                                       subsample=subsample,max_depth=max_depth),X,Y,cv=10,n_jobs=1,method='predict_proba')\n",
    "predict_save=[Y.astype(int),y_predict.astype(int),y_predict_prob[:,1]]\n",
    "predict_save=np.array(predict_save).T\n",
    "pd.DataFrame(predict_save).to_csv(path+outputname+\"_\"+classifier+'_predict.csv',header=None,index=False)\n",
    "ACC=metrics.accuracy_score(Y,y_predict)\n",
    "precision, recall, SN, SP, GM, TP, TN, FP, FN = performance(Y, y_predict)\n",
    "F1_Score=metrics.f1_score(Y, y_predict)\n",
    "F_measure=F1_Score\n",
    "MCC=metrics.matthews_corrcoef(Y, y_predict)\n",
    "pos=TP+FN\n",
    "neg=FP+TN\n",
    "savedata=[[[classifier+\"n_estimators:\"+str(n_estimators)+\"max_depth:\"+str(max_depth)+\"learning_rate:\"+str(learning_rate)+\"subsample:\"+str(subsample),ACC,precision, recall,SN, SP,\n",
    "            GM,F_measure,F1_Score,MCC,ROC_AUC_area,TP,FN,FP,TN,pos,neg]]]\n",
    "print savedata\n",
    "print X.shape[1]\n",
    "easy_excel.save(classifier+\"_crossvalidation\",[str(X.shape[1])],savedata,path+'cross_validation_'+classifier+\"_\"+outputname+'.xls')\n",
    "# y_predict_proba=cross_val_predict(svm.SVC(kernel='rbf',C=C,gamma=gamma,probability=True),X,Y,cv=10,method=\"predict_proba\")\n",
    "# Y=pd.DataFrame(Y)    \n",
    "# y_predict_proba=pd.DataFrame(y_predict_proba)\n",
    "# y_predict_proba=pd.concat([Y,y_predict_proba],axis=1)\n",
    "# pd.DataFrame(y_predict_proba).to_csv('/home02/chenhuangrong/RFH_ten_cross_validation_label.csv',header=None,index=False)\n",
    "# y_predict=pd.DataFrame(y_predict)\n",
    "# y_predict_=pd.concat([Y,y_predict],axis=1)\n",
    "# pd.DataFrame(y_predict_).to_csv('/home02/chenhuangrong/RFH_ten_cross_validation_predict.csv',header=None,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-43ca57e5d665>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mpositive_y_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpositive_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mnegative_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mnegative_x_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mnegative_y_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mpositive_negative_x_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpositive_x_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnegative_x_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1560\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Too many indexers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m                 raise ValueError(\"Location based indexing can only have [%s] \"\n\u001b[1;32m    153\u001b[0m                                  \"types\" % self._valid_types)\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_has_valid_type\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_valid_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_valid_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1531\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_is_valid_list_like\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1554\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import KFold  \n",
    "from sklearn import svm\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import math\n",
    "import easy_excel\n",
    "from sklearn.model_selection import *\n",
    "import sklearn.ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import sys\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "y_pred_prob_all=[]\n",
    "y_pred_all=[]\n",
    "Y_all=[]\n",
    "ACC_all=0\n",
    "precision_all=0\n",
    "recall_all=0\n",
    "SN_all=0\n",
    "SP_all=0\n",
    "GM_all=0\n",
    "TP_all=0\n",
    "TN_all=0\n",
    "FP_all=0\n",
    "FN_all=0\n",
    "F_measure_all=0\n",
    "F1_Score_all=0\n",
    "pos_all=0\n",
    "neg_all=0\n",
    "MCC_all=0\n",
    "\n",
    "def performance(labelArr, predictArr):\n",
    "    #labelArr[i] is actual value,predictArr[i] is predict value\n",
    "    TP = 0.; TN = 0.; FP = 0.; FN = 0.\n",
    "    for i in range(len(labelArr)):\n",
    "        if labelArr[i] == 1 and predictArr[i] == 1:\n",
    "            TP += 1.\n",
    "        if labelArr[i] == 1 and predictArr[i] == 0:\n",
    "            FN += 1.\n",
    "        if labelArr[i] == 0 and predictArr[i] == 1:\n",
    "            FP += 1.\n",
    "        if labelArr[i] == 0 and predictArr[i] == 0:\n",
    "            TN += 1.\n",
    "    if (TP + FN)==0:\n",
    "        SN=0\n",
    "    else:\n",
    "        SN = TP/(TP + FN) #Sensitivity = TP/P  and P = TP + FN\n",
    "    if (FP+TN)==0:\n",
    "        SP=0\n",
    "    else:\n",
    "        SP = TN/(FP + TN) #Specificity = TN/N  and N = TN + FP\n",
    "    if (TP+FP)==0:\n",
    "        precision=0\n",
    "    else:\n",
    "        precision=TP/(TP+FP)\n",
    "    if (TP+FN)==0:\n",
    "        recall==0\n",
    "    else:\n",
    "        recall=TP/(TP+FN)\n",
    "    GM=math.sqrt(recall*SP)\n",
    "    #MCC = (TP*TN-FP*FN)/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n",
    "    return precision,recall,SN,SP,GM,TP,TN,FP,FN\n",
    "seq=[]\n",
    "m6a_2614_sequence=\"data/huaman_AthMethPre.csv\"\n",
    "RNA_code='ACGU'\n",
    "k=1\n",
    "interval=1\n",
    "divided_num=10.0\n",
    "division_num=10\n",
    "seq=pd.read_csv(m6a_2614_sequence,header=None,index_col=None)\n",
    "seq=seq.values\n",
    "positive_seq=seq[0:1307,:]\n",
    "# X_train, X_test, y_train, y_test = cross_validation.train_test_split(train_data, train_target, test_size=0.1, random_state=0)\n",
    "negative_seq=seq[1307:2614,:]\n",
    "kf = KFold(n_splits=division_num,shuffle=False)  \n",
    "for train_index , test_index in kf.split(positive_seq):  \n",
    "    positive_df=pd.DataFrame(positive_seq)\n",
    "    positive_x_train=positive_df.iloc[train_index,:]\n",
    "    positive_y_train=positive_df.iloc[test_index,:]\n",
    "    negative_df=pd.DataFrame(negative_seq)\n",
    "    negative_x_train=negative_df.iloc[train_index,:]\n",
    "    negative_y_train=negative_df.iloc[test_index,:]\n",
    "    positive_negative_x_train=pd.concat([positive_x_train,negative_x_train],axis=0)\n",
    "    positive_negative_y_train=pd.concat([positive_y_train,negative_y_train],axis=0)\n",
    "    \n",
    "    X_train = np.array(positive_negative_x_train)\n",
    "    Y_train = list(map(lambda x: 1, xrange(len(positive_negative_x_train) / 2)))\n",
    "    Y2_train = list(map(lambda x: 0, xrange(len(positive_negative_x_train) / 2)))\n",
    "    Y_train.extend(Y2_train)\n",
    "    Y_train = np.array(Y_train)\n",
    "    \n",
    "    X_test = np.array(positive_negative_y_train)\n",
    "    Y_test  = list(map(lambda x: 1, xrange(len(positive_negative_y_train) / 2)))\n",
    "    Y2_test  = list(map(lambda x: 0, xrange(len(positive_negative_y_train) / 2)))\n",
    "    Y_test.extend(Y2_test )\n",
    "    Y_test  = np.array(Y_test)\n",
    "    print \"train and test samples finished\"\n",
    "    svc = svm.SVC(probability=True)\n",
    "    parameters = {'kernel': ['rbf'], 'C':map(lambda x:2**x,np.linspace(-2,5,14)), 'gamma':map(lambda x:2**x,np.linspace(-5,2,14))}\n",
    "    print \"begin grid search\"\n",
    "    clf = GridSearchCV(svc, parameters, cv=10, n_jobs=12, scoring='accuracy')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    C=clf.best_params_['C']\n",
    "    y_pred_prob=clf.predict_proba(X_test)\n",
    "    gamma=clf.best_params_['gamma']\n",
    "    print 'c:',C,'gamma:',gamma\n",
    "    y_pred=clf.predict(X_test)\n",
    "    \n",
    "    y_pred_prob_all.extend(y_pred_prob)\n",
    "    y_pred_all.extend(y_pred)\n",
    "    Y_all.extend(Y_test)\n",
    "#     ROC_AUC_area=\"%0.4f\"%cross_val_score(svm.SVC(kernel='rbf',C=C,gamma=gamma),X,Y,cv=10,n_jobs=-1).mean()\n",
    "    ACC=metrics.accuracy_score(Y_test,y_pred)\n",
    "    print ACC\n",
    "    precision, recall, SN, SP, GM, TP, TN, FP, FN = performance(Y_test, y_pred) \n",
    "    F1_Score=metrics.f1_score(Y_test, y_pred)\n",
    "    F_measure=F1_Score\n",
    "    MCC=metrics.matthews_corrcoef(Y_test, y_pred)\n",
    "    \n",
    "    pos=TP+FN\n",
    "    neg=FP+TN\n",
    "    ACC_all=ACC_all+ACC\n",
    "    print \"ACC_all:\",ACC_all\n",
    "    precision_all=precision_all+precision\n",
    "    recall_all=recall_all+recall\n",
    "    SN_all=SN_all+SN\n",
    "    SP_all=SP_all+SP\n",
    "    GM_all=GM_all+GM\n",
    "    TP_all=TP_all+TP\n",
    "    TN_all=TN_all+TN\n",
    "    FP_all=FP_all+FP\n",
    "    FN_all=FN_all+FN\n",
    "    F_measure_all=F_measure_all+F_measure\n",
    "    F1_Score_all=F1_Score_all+F1_Score\n",
    "    pos_all=pos_all+pos\n",
    "    neg_all=neg_all+neg\n",
    "    MCC_all=MCC_all+MCC\n",
    "all_y=[np.array(Y_all).astype(int),np.array(y_pred_all).astype(int),np.array(y_pred_prob_all).astype(list)[:,1]]\n",
    "pd.DataFrame(np.matrix(all_y).T).to_csv('/home02/chenhuangrong/AthMethPre_independent_test__.csv',header=None,index=False)\n",
    "fpr, tpr, thresholds = roc_curve(np.array(Y_all).T, list(np.array(y_pred_prob_all).astype(list)[:,1]))\n",
    "roc_auc = auc(fpr, tpr)\n",
    "savedata=[[['svm'+\"C:\"+str(C)+\"gamma:\"+str(gamma),ACC_all/divided_num,precision_all/divided_num, recall_all/divided_num,SN_all/divided_num,\n",
    "            SP_all/divided_num, GM_all/divided_num,F_measure_all/divided_num,F1_Score_all/divided_num,MCC_all/divided_num,roc_auc,TP_all,\n",
    "            FN_all,FP_all,TN_all,pos_all,neg_all]]]\n",
    "print savedata\n",
    "easy_excel.save(\"tenfold_crossvalidation\",[str(X_train.shape[1])],savedata,'/home02/chenhuangrong/AthMethPre_independent_test__.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
